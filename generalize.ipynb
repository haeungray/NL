{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ch2. Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "* 많은 문장들을 수집하여 단어와 단어 사이의 출현 빈도를 세어 확률을 계산함\n",
    "* 단어와 단어 사이의 확률을 계산하는데 불리하게 작용함 \n",
    "* 교착어작용 때문임  (확률이 퍼지게 됨) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  What is good model?\n",
    "* Generalization\n",
    "    모든 수에 대해서 학습 데이터를 모을 수 없으므로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count based Approximation\n",
    " * Given Sentence, 확률 분포를 모델링하고 싶을수도,\n",
    " * 체인룰을 이용해서 다음 단어에 대한 확률 예측 \n",
    " *  Apply Markov Assumption = 앞에 나온 k개의 토큰을 통해서 다음에 나올 단어를 근사함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram\n",
    " * n = k+1 \n",
    " * uni, bi, tri-gram으로 부름 \n",
    " * n이 커질수록 오히려 확률이 정확하게 표현되는데 어려움 \n",
    " * 보통은 3-gram을 가장 많이 사용함 \n",
    " * corpus의 양이 많을 때는 4-gram을 사용하기도함\n",
    "     * 언어모델의 성능은 크게 오르지 않는데 반해,\n",
    "     * 단어 조합의 경우의 수는 exponential하게 증가함\n",
    " * SRILM 라이브러리 존재하긴함\n",
    " * 확률값을 근사하는 가장 간단한 방법은 코퍼스에서 빈도를 세는 것, 복잡할수록 출현 빈도가 낮아 부정확한 근사가 이루어짐\n",
    " * 따라서 Markov assumption을 도입하여 확률값을 근사하자 학습코퍼스에서 보지 못한 문장에 대해서도 확률값을 구할 수 있음 (n의 크기가 중요함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
